# 一、MySQL 基础

## 1、主键

用来作**唯一标识**的字段

## 2、主键、外键、索引的区别

**定义**：

主键：唯一标识一条记录，不允许重复，不允许为空

外键：外键表示另一张表的主键，允许重复，可以是空值

索引：没有重复值，但可以有一个空值

**作用**：

主键：用来保存数据的完整性

外键：主要用于和其他表建立联系

索引：为了提高查询排序的速度

## 3、表的优化

**在建表之初就要考虑拆分逻辑**



**字段优化**：

1. 尽量使⽤ TINYINT，SMALLINT，MEDIUM_INT 替代 INT 类型，如果是非负则加上 UNSIGNED
2. VARCHAR 的长度只分配真正需要的空间
3. 尽量使⽤整数或者枚举替代字符串类型
4. 时间类型尽量使⽤ TIMESTAMP 而非 DATETIME
5. 单表不要放太多字段
6. 尽量少使用 NULL，很难查询优化而且占用额外索引空间

## 4、慢查询

**基础**：

1. 设置超时时间，超过这个时间的 sql 语句被称为慢查询
2. long_query_time：默认10秒

**优化**：

1. 运行语句，找到慢查询的 sql
2. 查询区分度最高的字段
3. explain：显⽰ mysql 如何使⽤索引来处理 select 语句以及连接表，可以帮助选择更好的索引、写出更优化的查询语句
4. order by limit 形式的 sql 语句，让排序的表优先查
5. 考虑建立索引
6. 了解业务使用场景

## 5、MySQL 与 MongoDB 的区别

**数据库模型**：

MySQL：关系型

MongoDB：非关系型

**存储方式**：

MySQL：不同引擎有不同的存储方式

MongoDB：虚拟内容+持久化

**查询语句**：

MySQL：sql 语句

MongoDB：使用 MongoDB 自己的查询语句，如 use、show、find、count、sort、insert、update、save、remove、limit

**常见架构特点**：

MySQL：单点、Master-Slave、MHA、Cluster

MongoDB：通过副本集与分片来实现高可用

**数据处理方式**：

MySQL：不同引擎有不同处理特点

MongoDB：基于内存，将热数据存储在物理内存中，从而达到高速读写的目的

**海量数据的存储**：

MySQL效率相对较低，但是 MongoDB 不支持事务

## 6、MySQL 表连接

### 6.1、内/外连接

**内连接**：

驱动表中的记录在被驱动表中找不到匹配的记录，那么驱动表的这条记录**不会加⼊到最后的结果中**

**外连接**：

驱动表中的记录在被驱动表中找不到匹配的记录，也**仍需要加⼊到最后结果中**

**注意**：

对于内连接，驱动表和被驱动表的顺序可以更换；对于外连接，这个顺序不能随意更换

### 6.2、过滤条件

**where**：

不论内外连接，只要是不符合 where 子句的记录都**不会**加⼊到最后的结果中

**on**：

在内连接中与 where 等价；

在外连接中，如果驱动表中的记录在被驱动表中没有记录可以匹配，该驱动表记录仍会加⼊到结果中，对应的被驱动表字段以 null 填充

### 6.3、嵌套循环连接

如果有3个表进⾏连接，那么表1和表2完成连接后的结果作为驱动表，将表3作为被驱动表进⾏连接查询

减少被驱动表的访问次数

### 6.4、索引

在根据驱动表的⼀条记录去被驱动表中查询时，相当于确定搜索条件的单表查询，可以使⽤索引优化单表查询

在被驱动表上使用二级索引进⾏查询时，可能连接查询的变量和过滤条件都是⼆级索引的部分列，可以不用回表直接覆盖索引，所以最好不要用 * 作为查询列表，而是使⽤真正需要查询的列

# 二、MySQL 锁

## 1、Innodb 使用表锁还是行锁？

大部分情况用行锁

**使用表锁的情况**：

1. 表比较大，事务需要更新全部或者大部分数据
2. 事务涉及到多个表，比较复杂，可能引起死锁，造成大量的事务回滚

## 2、锁的种类

### 2.1、全局锁

对**整个数据库实例**加锁。

**使用场景**：全库逻辑备份，即把整个库的表都 select 出来存成文本



MySQL 提供了⼀个加全局读锁的⽅法，命令是：

```mysql
Flush tables with read lock (FTWRL)
```

当需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：

1. 数据更新语句（数据的增删改）
2. 数据定义语句（包括建表、修改表结构等）
3. 更新类事务的提交语句

### 2.2、表级锁

有两种：

#### 2.2.1、表锁

**特点**：

1. 每次操作锁住整张表
2. 开销小，加锁快
3. 并发度最低

表锁的语法是：

```mysql
lock tables … read/write
```

与 FTWRL 类似，可以⽤ unlock tables 主动释放锁，也可以在客户端断开的时候⾃动释放

**注意**： lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象

#### 2.2.2、元数据锁（meta data lock，MDL）

MDL 不需要显式使⽤，在访问⼀个表的时候会被**自动加上**

**作用**：保证读写的正确性

- 当对⼀个表做增删改查操作的时候，加 MDL 读锁
- 当要对表做结构变更操作的时候，加 MDL 写锁

读锁之间不互斥，因此你可以有多个线程同时对⼀张表增删改查。读写锁之间、写锁之间是互斥的，⽤来保证变更表结构操作的安全性。因此，如果有两个线程要同时给⼀个表加字段，其中⼀个要等另⼀个执行完才能开始执行。

事务中的 MDL 锁，在语句执⾏开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。（这可能会产⽣死锁的问题）

#### 2.2.3、行锁

针对数据表中行记录的锁，也成为记录锁

**特点**：

1. 每次操作锁住一行数据
2. 开销大，加锁慢（因为要确定哪一行）
3. 发生锁冲突的概率是最低的，并发度是最高的

**两阶段锁协议**：在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要到事务结束时才释放

### 2.3、加锁规则

两个“原则”、两个“优化”、一个“bug”

**两个原则**：

1. 加锁的基本单位是 next-key lock。next-key lock 是**前开后闭**区间
2. 查找过程中，访问到的对象才会加锁

**两个优化**：

1. 索引上的等值查询，给唯⼀索引加锁的时候，next-key lock 退化为**行锁**
2. 索引上的等值查询，向右遍历时且最后⼀个值不满足等值条件的时候，next-key lock 退化为**间隙锁**

**一个“bug”**：

1. 唯⼀索引上的范围查询会访问到不满足条件的第⼀个值为止。（MySQL8.0之后的版本已经修复）



以上规则是在**可重复读隔离级别** (repeatable-read) 下验证的。同时，可重复读隔离级别遵守两阶段锁协议，所有加锁的资源，都是在事务提交或者回滚的时候才释放的。如果切换到**读提交隔离级别** (read-committed) 的话，就好理解了，过程中去掉间隙锁的部分，也就是只剩下行锁的部分。

### 2.4、锁的划分

#### 2.4.1、数据库角度

##### 2.4.1.1、共享锁（读锁、S 锁）

共享锁锁定的资源可以被其他⽤户读取，但不能修改

在进⾏ SELECT 的时候，会将对象进⾏共享锁锁定，当数据读取完毕之后，就会释放共享锁，这样就可以保证数据在读取时不被修改

**例子**：

```mysql
SELECT user_id FROM product_comment WHERE user_id = 10 LOCK IN SHARE MODE;
```

##### 2.4.1.2、排他锁（写锁、X 锁）

排它锁锁定的数据只允许**进⾏锁定操作的事务**使⽤，其他事务⽆法对已锁定的数据进⾏查询或修改

**例子**：

```mysql
SELECT user_id FROM product_comment WHERE user_id = 10 FOR UPDATE;
```

另外当对数据进⾏更新的时候，也就是 INSERT、DELETE 或者 UPDATE 的时候，数据库也会**自动**使⽤排它锁，防⽌其他事务对该数据⾏进⾏操作

##### 2.4.1.3、共享锁与排他锁

不仅可以锁住⼀⾏，也可以锁住⼀张表

**例子**：

```mysql
// 加读锁
LOCK TABLE product_comment READ;
// 解锁读锁
UNLOCK TABLE;

// 加写锁
LOCK TABLE product_comment WRITE;
// 解锁写锁
UNLOCK TABLE;
```

##### 2.4.1.4、意向锁（Intent Lock）

给更⼤⼀级别的空间⽰意⾥⾯**是否已经上过锁**

**例子**：如果我们给某⼀⾏数据加上了排它锁，数据库会⾃动给更⼤⼀级的空间，⽐如数据页或数据表加上意向锁，告诉其他⼈这个数据页或数据表已经有⼈上过排它锁了，这样当其他⼈想要获取数据表排它锁的时候，只需要了解是否有⼈已经获取了这个数据表的意向排他锁即可

如果事务想要获得数据表中某些记录的共享锁，就需要在数据表上添加意向共享锁。同理，事务想要获得数据表中某些记录的排他锁，就需要在数据表上添加意向排他锁。

#### 2.4.2、程序员角度

##### 2.4.2.1、乐观锁

认为对同⼀数据的并发操作不会总发⽣，属于⼩概率事件，不⽤每次都对数据上锁，也就是**不采⽤数据库⾃⾝的锁机制**，⽽是通过程序来实现。在程序上，我们可以采⽤版本号机制或者时间戳机制实现。

##### 2.4.2.2、悲观锁

对数据被其他事务的修改持保守态度，**会通过数据库⾃⾝的锁机制来实现**，从⽽保证数据操作的排它性。

##### 2.4.2.3、乐观锁和悲观锁的适用场景

**乐观锁**：

读多写少

优点在于程序实现，不存在死锁问题，不过适⽤场景也会相对乐观，因为它阻⽌不了除了程序以外的数据库操作

**悲观锁**：

写操作多

并发访问性不好

# 三、MySQL 事务

## 1、基本概念

1. 一个最小的不可再分的工作单元
2. 通常情况下，一个事务对应一个完整的业务

## 2、MySQL 事务特性

ACID

### 2.1、A 原子性

⼀个事务的所有操作，要么全部完成，要么都没完成，不能结束在中间环节。如果事务在执⾏过程中发⽣错误，会被回滚到事务开始之前的状态

### 2.2、C 一致性

在事务开始之前以及事务结束之后，数据库的完整性不能被破坏

### 2.3、I 隔离性

允许多个并发事务同时对数据进⾏修改和读写的能⼒，它可以防⽌由于多个事务并发执⾏时由于交叉执⾏⽽导致的数据不⼀致

### 2.4、D 持久性

事务处理结束了以后，对数据的修改是永久的，即使是发⽣了系统故障，数据也不会丢失

## 3、MySQL 四大隔离级别

**脏读**：一个事务读取另一个事务还没有提交的数据

**不可重复读**：同一个事务内，两个相同的查询返回了不同的结果

**幻读**：在一次事务中，前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行

### 3.1、RU 读不提交（Read Uncommited）

在该隔离级别下，事务之间完全不隔离，**会产⽣脏读**，⼀般情况不会使⽤

### 3.2、RC 读提交（Read Commited）

本事务读取到的是其它事务提交的最新数据，但有⼀个问题，在同⼀个事务中，前后两相同的 select 可能会读到不同的结果（幻读）

### 3.3、RR 可重复读（Repeatable Read）

在同⼀个事务中，select 的结果是事务开始时时间点的状态，因此，同⼀个事务同样的 select 操作可以读到⼀致的结果

### 3.4、serializable 串行化

读操作会隐式获取共享锁，保证不同事务之间的互斥

# 四、分布式

## 1、幂等操作

⽤户对同⼀操作发起⼀次请求或者多次请求的结果是⼀致的，不会因为多次点击⽽产⽣副作⽤

## 2、lvs 负载均衡

Linux Virtual Server：⼀个实现负载均衡的开源软件项⽬

**目标**：通过 LVS 的负载均衡技术与 Linux 操作系统实现一个高性能，高可用的 Linux 服务器集群

### 2.1、逻辑架构

1. 调度层
2. server 集群层
3. 共享存储

### 2.2、基本工作原理

**IPVS**：使用户定义的策略生效

**ipvsadn**：用于用户定义和集群服务管理的工具



**原理**：

IVS 的 IP 负载均衡技术主要通过 IPVS 实现，IPVS 虚拟⼀个 IP 地址，⽤户必须通过这个虚拟的 IP 地址访问服务器。该虚拟 IP 被称为LVS 的 VIP，访问的请求⾸先经过 LVS 的 VIP，到达负载调度器，由负载调度器从 real server 列表中选取⼀个服务节点响应⽤户的请求

## 3、lvs 和 Nginx 的区别

**负载均衡**：

LVS：4层负载均衡

Nginx：7层负载均衡

**技术原理**：

LVS：IP+TCP 端口

Nginx：URL 应用层（内容交换）

**优点**：

LVS：配置简单，效率高

Nginx：智能化，可以对客户端请求服务端响应进⾏⾃定义修改。极⼤的提升了应⽤系统在⽹络层的灵活性

**缺点**：

LVS：不理解http，ftp等应⽤协议，满⾜不了特定需求，⽐如动静分离等

Nginx：对负载均衡的设备要求很⾼，配置复杂

## 4、consul、etcd 区别

分布式微服务架构：一个应用可能由一组职责单一的服务组成

consul、etcd 都提供了相关的服务管理功能

### 4.1、区别

**服务健康检查**：

consul：服务状态、内存、硬盘等

etcd：连接心跳

**多数据中心**：

consul：支持

etcd：不支持

**使用接口**：

consul：http/dns

etcd：http/grpc

## 5、分布式事务（重点）

保证在分布式环境中，不同的数据的数据一致性

## 6、分布式锁（重点）

将多线程的锁机制应用到分布式的部署环境中

### 6.1、特点

1. 互斥性
2. 可重入：同一个节点上的同一个线程，获取到锁之后还可以再次获取这个锁
3. 锁超时：支持锁超时，防止死锁
4. 高效，高可用
5. 支持阻塞和非阻塞
6. 支持公平锁和非公平锁

### 6.2、常见分布式锁

1. MySQL
2. Redis
3. Zookeeper

## 7、淘汰算法

### 7.1、LRU（Least Recently Used）

**概念**：

最近最少使用，如果数据最近被访问过，那么将来被访问的几率也高

**实现**：

1. 设计一个链表来缓存数据
2. 新数据插入到链表头部
3. 每当缓存命中（也就是缓存数据被访问），将数据移动到链表头部，当链表满的时候，将链表尾部的数据丢弃

### 7.2、LFU（Latest Frequently Used）

**概念**：

最不经常使⽤，如果⼀个数据在最近⼀段时间内使⽤次数很少，那么它在将来的⼀段时间内被使⽤的可能性也很⼩

**实现**：

1. LFU 每个数据块设计一个引用计数
2. 所有数据块按照引用计数排序，具有相同引用计数的数据块按照时间排序
3. 新加入的数据插入到队列尾部
4. 在队列中某一个数据被访问，该数据引用计数加1，队列重新排序需要删除数据时，将已排序的列表最后的数据块删除

### 7.3、FIFO（First In First Out）

**概念**：

先进先出，如果⼀个数据最先进⼊缓存中，就最早淘汰掉

**实现**：

直接用队列实现

## 8、一致性 hash 算法

**作用**：

该算法通常⽤于负载均衡中要求资源被均匀的分布到所有节点上，并且对资源的请求能够快速路由到对应的节点上⾯

**特点**：

1. 平衡性：哈希产生的结果要均匀的分配在整个输出空间中
2. 单调性：当数据发⽣变动的时候，对相同的数据始终映射到相同的节点，或者新增加的缓冲节点中，避免⽆法找到原来的数据
3. 稳定性：当出现节点坏掉或者需要动态扩容时，尽量减少数据的移动

**原理**：

1. 哈希环：将整个哈希的输出空间（2^32）设置为⼀个环形区域
2. 设置哈希环
3. 将服务器进⾏哈希，可以考虑使⽤服务器的编号或者ip等作为输⼊，得到⼀个输出值，将该输出值映射到输出空间的环形区域上
4. 对⽤户数据进⾏同样的hash操作，映射在环形区域上。然后让数据按照顺时针⽅向移动，遇到的第⼀个服务器就是它分配的服务器
5. 通过将整个哈希输出空间设置为⼀个环形区别，可以有效的减⼩输出空间的变化对于哈希结果的影响

## 9、CAP 原理

### 9.1、基本概念

1. C（一致性）：对于分布在不同节点上的数据，如果⼀个节点更新数据之后，其他节点都能读到这个最新的数据
2. A（可⽤性）：非故障节点在合理的时间内返回合理的响应
3. P（分区容错性）：出现网络错误，系统还能继续⼯作

### 9.2、CAP 不能同时共存

**原因**：

对于CAP理论中，分布式系统要保障整体的服务，因此**分区容错性（P）必然存在**。那么为什么CA不能同时存在？因为分区之间的通信可能通信失败

![image-20240311173030831](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202403111730929.png)



CA：分布式中一般不会选择

CP：CP的代表是Zookeeper，放弃可⽤性，追求⼀致性和容错性

AP：追求分区容错性与可⽤性

## 10、BASE 理论

**Basically Available（基本可⽤）**：分布式系统在出现故障时，允许损失部分可⽤功能

**Soft State（软状态）**：允许系统中存在**中间状态**，这个状态不影响系统可⽤性

**Eventually consistent（最终⼀致性）**：在经过⼀段时间之后，所有节点的数据都会达到⼀致

# 五、MySQL 索引

索引的出现其实就是为了提⾼数据查询的效率，就像书的⽬录⼀样



**MySQL 在查询方面主要就是两种方式**：

1. 全表扫描（一个一个挨个找）
2. 根据索引检索



**创建索引**：

```mysql
create index 索引名 on 表名(列名);
```

**删除索引**：

```mysql
drop index 索引名 on 表名;
```

## 1、在 MySQL 当中，怎么查看一个 SQL 语句是否使用了索引进行检索？

1. 在 SQL 语句前，添加 explain 关键字
2. 当 type=ALL 时，表示使用**全表查询**（未使用索引）
3. 当 type=RES 时，表示使用索引

## 2、索引失效的情况

1. 模糊匹配当中以 “%” 开头时，索引失效
2. OR 有⼀边的条件字段没有索引时，索引失效
3. 使⽤复合索引的时候，没有使⽤左侧的列查找，索引失效
4. 在 where 当中索引列参加了运算，索引失效
5. 在 where 当中索引列使⽤了函数，索引失效

## 3、注意事项

1. 在任何数据库当中**主键**上都会⾃动添加索引对象（聚簇索引）
2. 在 mysql 当中，⼀个字段上如果有 unique 约束的话，也会⾃动创建索引对象
3. 索引不是越多越好，虽然索引会提⾼ select 效率，但是也降低了 insert 以及 update 的效率
4. 数据量小的表不需要建⽴索引，会增加额外的索引开销
5. 不经常使⽤的列不要建⽴索引
6. 频繁更新的列不要建⽴索引，会影响更新的效率

## 4、MySQL 的索引有几种

1. 普通索引：最基本的索引，没有任何限制
2. 唯⼀索引：与普通索引类似，但索引列的值必须是唯⼀的，允许空值
3. 主键索引：⼀种特殊的唯⼀索引，⼀个表只能有⼀个主键，不允许有空值
4. 组合索引：在多个字段上创建的索引，只有在查询条件中使用了**创建索引的第⼀个字段**，索引才会被使用
5. 全⽂索引：主要⽤来**查找⽂本中的关键字**，类似于搜索引擎

## 5、索引优化

1. 尽量避免在 where 字句中对字段进⾏空值判断，这会导致引擎放弃使⽤索引，进⾏全表扫描
2. 字段值分布很稀少的字段，不适合建⽴索引
3. 不要⽤字符字段做主键
4. 字符字段只建⽴前缀索引
5. 不要⽤外键和UNIQUE
6. 使⽤多列索引时，注意顺序和查询条件保持⼀致，同时删除不必要的单列索引

## 6、索引常见类型

### 6.1、哈希表

**优点**：

key ⽆序，插⼊数据时⽆需维护顺序（直接在最后⼀个元素后追加，即可），效率较⾼

**缺点**：

因为不是有序的，所以哈希索引做区间查询的速度是很慢

**适用场景**：

适⽤于只有等值查询的场景，⽽不适⽤频繁区间查找

### 6.2、有序数组

**优点**：

等值查询 和 区间查询 性能都挺6 （有序数组 适合 **查询**）

**缺点**：

有序数组不适合 频繁 增/删 记录的场景 （但是，有序数组**不合适 增/删**）

**适用场景**：

有序数组只适⽤于**静态（固定长度）存储引擎**，在 等值查询 和 范围（区间）查询 场景中的性能⾮常优秀

### 6.3、二叉搜索树

BST 是为了 保留了 “有序数组” 查询（⼆分，O(logN)）性能好的优点，同时解决“有序数组” 不适合 增/删 的缺点

⼆叉树是搜索效率最⾼的，但是实际上⼤多数的数据库存储却并不使⽤⼆叉树。其原因是，索引不⽌存在内存中，还要写到磁盘上

### 6.4、N 叉树

为了让⼀个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块

即，通过使⽤ N 叉树 来降低 树的⾼度，即 减少读取磁盘的次数（IO是很慢的），提⾼查询效率

### 6.5、InnoDB 索引模型

InnoDB 使⽤了 B+ 树索引模型，**每⼀个索引**在 InnoDB ⾥⾯对应**⼀棵 B+ 树**

假设建表语句如下，则对应的 2 棵 索引树（主键 ⾃动创建⼀颗；字段 k 创建 ⼀颗索引
树），如下所示：

![image-20240311204057765](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202403112040815.png)

![image-20240311204119979](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202403112041023.png)

### 6.6、聚簇索引和非聚簇索引

根据叶⼦节点的内容，索引类型分为主键索引和⾮主键索引

主键索引的叶⼦节点存的是整⾏数据在 InnoDB ⾥，主键索引也被称为**聚簇索引**（clustered index）

⾮主键索引的叶⼦节点存的是主键的值在 InnoDB ⾥，非主键索引也被称为**二级索引**（secondary index）

## 7、主键查询 vs 普通索引查询

**主键查询**：

直接在 主键索引 所在的 B+ 树中查询，然后直接返回查询到的叶⼦节点（此时，叶⼦节点⾥⾯就是整⾏记录）

**普通索引查询**：

⾸先，在普通索引所在的 B+ 树中，查询到待查询记录的 主键；然后，再根据这些查到的 主键，执⾏ “主键查询” （即，回表查询）

## 8、索引维护

B+ 树为了维护索引有序性，如果待插⼊记录所在的数据页已经满了，则可能造成 “页分裂”（申请⼀个新的数据页，然后将部分数据 挪过去）

**页分裂的缺点**：

1. 影响系统性能
2. 影响数据页的利⽤率
3. 主键长度越⼩，普通索引的叶⼦节点就越⼩，普通索引占⽤的空间也就越⼩
4. 所以，应该尽量使⽤ ⾃增主键，减少空间存储消耗。（但是，事⽆绝对，只是⼤多情况使⽤ ⾃增主键，下⾯也有特例）

**适合用业务字段 直接做主键，而不是使用自增主键的场景特例**：

1. 只有一个索引
2. 该索引必须是唯一索引

## 9、覆盖索引

在**普通索引树**中，就可以直接查到待查结果，⽽不需要回表



当执⾏如下语句时，这时只需要查 ID 的值，⽽ ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表

```mysql
select ID from T where k between 3 and 5;
```

由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使⽤覆盖索引是⼀个常⽤的**性能优化手段**

## 10、最左前缀原则

**场景**：

当单独为⼀个不频繁的请求创建索引时，会造成空间浪费；但是，如果直接让它⾛全表查询(⼀个⼀个找)，那效率也太低了吧？此时，应该怎么做？

答：B+ 树这种索引结构，可以利⽤索引的“最左前缀”，来定位记录。

![image-20240311205700786](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202403112057837.png)

**使用“最左前缀”查询**：

当查询语句是 where name like ‘张 %’时，也能够⽤上这个索引

1. 首先，查找到第⼀个符合条件的记录是 ID3
2. 然后，向后遍历，直到不满⾜条件为止

但是，使⽤ 'like %张" 时，索引失效（即，必须使⽤ 最左前缀）

最左前缀可以是**联合索引**的最左 N 个字段，也可以是字符串索引的最左 M 个字符



**Note**：

考虑到“最左前缀原则”，可以通过调整组合索引中的字段顺序，可以少维护⼀个索引

## 11、索引下推

![image-20240311210018738](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202403112100784.png)

根据前缀索引规则，这个语句在搜索索引树的时候，只能⽤ “张”，找到第⼀个满⾜条件的记录 ID3；然后在 MySQL 5.6 之前，只能从 ID3 开始⼀个个回表，到主键索引上找出数据⾏，再对⽐字段值。



**概念**：

在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满⾜条件的记录，减少回表次数

![image-20240311210518005](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202403112105045.png)

![image-20240311210506293](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202403112105347.png)

## 12、索引的优缺点

**优点**：

使⽤索引可以⼤⼤加快数据的检索速度（⼤⼤减少检索的数据量），这也是创建索引的最主要的原因

通过创建唯⼀性索引，可以保证数据库表中每⼀⾏数据的唯⼀性

**缺点**：

空间消耗，⼀个索引对应的就是⼀棵 b+树，每⼀个节点都是⼀个 16KB ⼤⼩的页。占⽤的空间较⼤

创建索引和维护索引需要耗费许多时间，当对表中的数据进⾏增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低 SQL 执⾏效率

# 六、MySQL 用户管理

**目的**：

为了保证 MySQL 服务器的安全，每个 MySQL 的⽤户应该对他们需要的数据具有适当的访问权



（暂时不写）

# 七、MySQL 语句执行

## 1、查询语句运行

### 1.1、连接器

负责跟客户端建⽴连接、获取权限、维持和管理连接



Mysql 会定期⾃动清理"空闲"连接，由参数 wait_timeout 控制的，默认值是 8 ⼩时。由于建⽴连接⽐较复杂，所以**尽量使⽤长连接**，⽽不是 短连接（少量查询后，就断开连接）但是，当 长连接 过多时，可能导致内存占⽤太⼤，被系统强⾏杀掉（OOM），即会导致MySQL 异常重启

**解决方案**：

1. 定期断开长连接
2. 5.7版本后，可以通过执行 mysql_reset_connection 来重新初始化连接资源

### 1.2、查询缓存

MySQL 8.0 版本直接将查询缓存的整块功能删掉

### 1.3、分析器

1. 词法分析
2. 语法分析

### 1.4、优化器

优化器是在表⾥⾯有多个索引的时候，决定使⽤哪个索引；或者在⼀个语句有多表关联（join）的时候，决定各个表的连接顺序

### 1.5、执行器

执⾏语句时，⾸先判断当前⽤户是否有执⾏权限

- 如果没有，则终止
- 如果有，则执行器就会根据表的引擎定义，去使用这个引擎提供的接口

## 2、更新语句执行

### 2.1、redo log VS. binlob

1. redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用
2. redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑⽇志，记录的是这个语句的原始逻辑，⽐如”给 ID=2 这⼀⾏的 c 字段加 1“
3. redo log 是循环写的，空间固定会⽤完；binlog 是可以追加写⼊的。“追加写”是指 binlog ⽂件写到⼀定⼤⼩后会切换到下⼀个，并不会覆盖以前的⽇志

### 2.2、数据恢复

1. 找到最新的全量备份，进行恢复
2. 然后依次执行该全量备份以后的所有 binlog（增量备份）

### 2.3、为什么必须有“两阶段提交”？

这是为了让两份⽇志之间的逻辑⼀致性

![image-20240311213706248](https://raw.githubusercontent.com/1793925850/user-image/master/imgs/202403112137297.png)

# 八、MySQL 查询优化

查询优化就是 MySQL 会将程序员编写的⼀些比较耗费性能的语句进行**查询重写**

## 1、条件化简

## 2、移除不必要的括号

## 3、常量传递

某个表达式是某个列和某个常量的等值匹配，会直接用常量替换列名

## 4、移除没用的条件

比如永远为 true 或 false

## 5、表达式计算

1. 表达式只包含常量的话，值会被计算出来
2. 如果某个列在函数中或者以运算形式出现，优化器不会进行化简

## 6、having 和 where 子句的合并

1. 查询语句中没有 sum、max 这样的聚集函数以及 group ⼦句的话，优化器会将 having 和 where ⼦句合并
2. having 子句用于分组后过滤，where 用于分组前合并

## 7、常量表检测

1. 使用主键等值匹配、使用唯一二级索引列等值匹配进行查询的表称为常量表
2. 优化器会优先执行常量表查询，因为速度非常快

## 8、外连接消除

## 9、优化器会将右连接转换为左连接查询

## 10、空值拒绝

1. 在外连接查询中，指定的 where ⼦句中包含被驱动表的列不为null值的条件（就是不允许查出来的记录中含有null值）
2. 被驱动表的 where ⼦句符合空值拒绝的条件后，外连接和内连接可以相互转换

## 11、子查询优化

## 12、子查询简介

在⼀个查询语句中的某个位置可以出现另⼀个查询语句，这另⼀个查询就叫**子查询**

### 12.1、按出现位置分类

- 在 select 子句中

  ```mysql
  select (select m1 from t1 limit 1);
  ```

- **在 from 子句中**

  ```mysql
  select m, n from (select ...);
  ```

  将这种子查询的结果当作一个表，在 from 子句中的子查询成为派生表

- **在 where 或 on 子句中**

  ```mysql
  select * from t1 where m1 in (select ...);
  ```

### 12.2、按返回的结果集分类

- 标量子查询：只返回一个单一值

  ```mysql
  select (select m1 from t1 limit 1);
  ```

- **行子查询：**返回一条记录，需要包含多个列。使用 limit 1 保证子查询的结果只有一条记录

  ```mysql
  select * from t1 where (m1, n1) = (select m2, n2 from t2 limit 1);
  ```

- 列子查询：返回一个列的数据，可能包含多条记录

  ```mysql
  select * from t1 where m1 in (select m2 from t2);
  ```

- **表子查询：**子查询的结果既有多条记录，又有多个列

  ```mysql
  select * from t1 where (m1, n1) = (select m2, n2 from t2);
  ```

### 12.3、按外层查询关系分类

- **不相关子查询：**

  子查询可以单独运行出结果，不依赖于外层查询的值（上面都是例子）

- **相关子查询：**

  子查询的执行需要依赖于外层查询的值

  ```mysql
  select * from t1 where m1 in (select m2 from t2 where n1 = n2);
  ```

### 12.4、子查询的注意事项

1. 必须用小括号括起来
2. select 子句中的子查询必须是标量子查询
3. 要想得到标量⼦查询/⾏⼦查询，应该使⽤ limit 1
4. 对于 [not] in/any/some/all ⼦查询来说，⼦查询不允许出现 limit
5. ⼦查询中不必使⽤ order by：⼦查询相当于⼀个集合，集合没必要排序
6. ⼦查询中不必使⽤ distinct，因为集合也不需要去重
7. ⼦查询中没有聚集函数以及 having ⼦句时，不必使⽤ group by ⼦句

## 13、子查询的执行方式

1. 不优化

   - 对于不相关⼦查询，先执⾏⼦查询，再将⼦查询的结果作为外层查询的参数
   - 对于相关⼦查询，先从外层查询中取⼀条记录，取相关列进⾏⼦查询，如此循环

2. in 子查询的优化

3. 物化

   将⼦查询的结果写⼊临时表中，该临时表就是物化表

   基于内存的物化表建⽴**哈希索引**，基于磁盘的物化表建⽴ **B+ 树索引**

4. 半连接

   s1 表和 s2 表半连接的意思是：对于 s1 表的某条记录，只关⼼在 s2 表中是否存在与之匹配的记录，⽽不关⼼与多少条记录与之匹配（mysql 内部执行子查询的一种方式，不面向用户）

5. 实现方式

6. table pullout（表上拉）

   查询列表只有主键或唯⼀索引列是，将⼦查询中的表上拉到外层查询的 from ⼦句中

7. 重复值消除

   1. 用临时表消除半连接结果集的重复值
   2. 松散扫描
   3. 半连接物化

8. 首次匹配？

# 九、MySQL 高性能

## 1、MySQL 主从复制

### 1.1、概念

数据可以从⼀个 MySQL 数据库服务器主节点复制到⼀个或者多个从节点。

MySQL 默认采⽤**异步复制**⽅式，这样从节点就不⽤⼀直访问主服务器来更新最新数据。

从节点可以复制主节点数据库中的所有数据库、特定的数据库或者特定的表。

### 1.2、用途

#### 1.2.1、数据实时备份

当系统中某个节点发⽣故障时，可以⽅便故障切换

#### 1.2.2、读写分离

在开发过程中，如果遇到某个 sql 语句需要锁表，导致暂时不能使⽤读的服务

使⽤主从复制，让主数据库负责写，从数据库负责读，即使主库出现锁表的情景，也可以通过从库正常读数据

#### 1.2.3、架构扩展

随着系统中业务访问量的增加，如果是单机部署数据，会导致 I/O 访问频率过⾼

通过主从复制，增加多个数据存储结点，将负载分布在多个从节点上，降低单机的 I/O 访问频率，提⾼单机的 I/O 性能

### 1.3、原理

MySQL 主从复制涉及到三个线程

一个运行在主节点：**binary log dump** thread

两个运行在从节点：**I/O thread**、**SQL thread**

#### 1.3.1、binary log dump

当从节点连接主节点的时候，主节点创建该线程，用于发送 bin-log 内容

#### 1.3.2、I/O thread

当从节点执⾏“start slave”命令之后，从节点会创建⼀个 I/O 线程⽤来连接主节点，请求其中的数据。I/O 线程接收到主节点 binlog dump 的更新数据之后，保存在本地的 relay log 中

#### 1.3.3、SQL thread

该线程负责读取 relay log 中的内容，解析具体的操作并执⾏，最终保证主从数据的⼀致性

### 1.4、基本过程

1. **从节点 I/O 进程连接主节点**：请求指定日志文件的指定位置后面的内容
2. **主节点接收到请求之后**：通过负责复制的 I/O 进程根据请求的信息读取指定的⽇志位置**之后**的⽇志信息，返回给从节点。返回信息中除了⽇志所包含的指定⽇志信息还包含了本次返回信息的 **bin-log file** 以及 **bin-log position**
3. **从节点的 I/O 线程接收到内容之后**：将接收到的⽇志内容更新到本机的 **relay log** 中，并且把读取到的 binary log ⽂件名和位置保存到 master-info ⽂件中，⽅便下⼀次告知 master 从节点需要更新的位置
4. **Slave 的 SQL 线程检测到 relay-log 中新增了内容**：将 **relay-log** 的内容解析成在主节点上实际执⾏的操作，并在数据库中执⾏

### 1.5、主从复制模式

1. 异步模式
2. 半同步模式
3. 全同步模式
4. GTID 复制模式

## 2、分库分表

### 2.1、原因

1. **单库太大**

   单个数据库处理能⼒有限，所在的服务器上的磁盘空间也有限，单库存在 I/O 操作瓶颈

   主要方案：切分成更多更小的库

2. **单表太大**

   CURD 都成问题，索引膨胀，查询超时

   主要方案：切分成多个数据集更小的表

### 2.2、拆分方案

#### 2.2.1、垂直拆分

1. **垂直分表**
   1. “大表拆小表”，基于列的字段进行
   2. 一般表中字段较多，将不常用的、数据较大的、长度长的拆分到“扩展表”
2. **垂直分库**
   1. 一般情况下，针对一个系统中不同业务进行拆分
   2. 一般拆分之后，放到多个服务器上

#### 2.2.2、水平拆分

1. **水平分表**

   针对数据量巨大的单张表（比如订单），按照某种规则（range、hash 取模），切分到多张表中，这些表还在一个数据库中

2. **水平分库**

   将单张表的数据切分到多个服务器上，每个服务器都有相应的库和表，只是表中的数据集合不同

   ⽔平分库能够有效的缓解单机和单库的性能瓶颈，I/O，连接数和硬件资源等瓶颈



**水平分库分表切分规则**

- **range**：根据范围，比如 0-1000 一个表，1001 到 2000 一个表
- **hash 取模**：⽐如取 ID，进⾏ hash 取模，根据模数分配到不同的数据库中
- **地理区域**：按照地理范围进行划分
- **时间范围**：按照时间进行切分

## 3、MySQL 高可用方案类型

1. **基于主从复制**
   1. 一般情况下，采用双节点主从+ keepalived/heartbeat 方案
   2. 在 master 节点发生故障以后，复用 keepalived/heartbeat 的高可用机制实现快速切换到 slave 节点
2. **基于 Galera 协议**
3. **基于 NDB 协议**
4. **基于中间件 /proxy**
5. **基于共享存储**
6. **基于主机高可用**

# 十、MySQL 事务隔离

MySQL 原⽣引擎 MyISAM 不⽀持事务，所以被 InnoDB 取代



事务， **ACID**（**Atomicity**、**Consistency**、**Isolation**、**Durability**，即原⼦性、⼀致性、隔离
性、持久性）

## 1、redis 的隔离性

redis 的事务中的隔离性并没有保证原⼦性，已经做过的操作是不会 rollback 的。

它的隔离性是指**其他事务不会干扰到它自己的事务**

## 2、隔离性与隔离级别

详情查看第三章第3节

## 3、数据库对隔离级别的实现

DB 会创建一个视图

- **可重复读**
  - 视图在事务启动时创建，整个事务存在期间都用这个视图
- **读已提交**
  - 视图在每个 SQL 语句开始执行的时候创建
- **读未提交**
  - 直接返回记录上的最新值，没有视图概念
- **串行化**
  - 直接用加锁的方式来避免并行访问

## 4、Oracle 默认

读已提交

MySQL 中的设置：transaction-isolation 设置为 read-committed，⽤ showvariables 来查看



**可重复读的场景**：

校对上个⽉的余额和这个⽉余额的差值，你希望在校对过程中，即使有⽤户发⽣了新的交易，也不影响校对结果。

## 5、事务隔离的实现

在 MySQL 中，实际上每条记录在更新的时候都会同时记录⼀条**回滚操作**。记录上的最新值，通过回滚操作，都可以得到前⼀个状态的值。

同⼀条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（**MVCC**）

## 6、事务的启动方式

**显式启动事务语句**

```mysql
begin/start transaction、commit、rollback
```

**关闭线程的自提交**

```mysql
set autocommit=0;
```

事务会持续存在直到你主动执⾏commit或者rollback，或者断开连接，所以，如果采取了第⼆种⽅法，就导致了接下来的查询都在事务中，如果是长连接，就导致了长事务。

建议使用 `set autocommit=1`



**长事务的查询**：

在 informationschema 库下的 innodbtrx 表中查询

## 7、如何避免长事务对业务的影响？

**从应用开发端来看**：

1. 确定是否使⽤了set autocommit=0，如是，则改成1
2. 确定是否有不必要的只读事务
3. 业务连接数据库的时候，控制每个语句执⾏的最长时间 set maxexecutiontime

**从数据库端来看**：

1. 监控相关表，设置长事务阈值，超过就报警或 kill
2. 可使⽤ percona 的 pt-kill ⼯具
3. 在功能测试阶段输出所有的 log，分析⽇志提前发现问题
4. 把 innodbundotablespaces 设置成2或者更⼤的值

# 十一、MySQL 常见问题

## 1、什么是幻读？

幻读是指在同⼀个事务中，存在前后两次查询同⼀个范围的数据，但是第⼆次查询却看到了第⼀次查询没看到的⾏。



**幻读出现的场景**：

1. 事务隔离级别为可重复读，且是当前读
2. 幻读仅专指新插入的行

## 2、幻读带来的问题？

1. 对行锁语义的破坏
2. 破坏了数据一致性

## 3、怎么避免幻读？

存储引擎加间隙锁

## 4、为什么会出现幻读？

行锁只能锁定存在的行，针对新插入的操作没有限定

## 5、间隙锁是啥？它怎么避免出现幻读的？它引入了什么新的问题？

**间隙锁**：

是专门⽤于解决幻读这种问题的锁。它锁了⾏与⾏之间的间隙，能够阻塞新插⼊的操作间隙锁的引⼊也带来了⼀些新的问题，⽐如：降低并发度，可能导致死锁。

另外，间隙锁在可重复读级别下才是有效的。

## 6、间隙锁和 next-key lock

行锁和间隙锁合称 next-key lock，这个锁是左开右闭区间。



MySQL 为了解决幻读问题，在线程更新数据并 next-key lock 的过程中，⾸先必须在**可重复读**的隔离级别下，执⾏以下的原则和优化：

**原则**：

1. 加锁的基本单位是 next-key lock，next-key lock 是前开后闭区间
2. 查找过程中访问到的对象才会加锁

**优化**：

1. 索引上的等值查询，给唯⼀索引加锁的时候，next-key lock 退化为⾏锁，如果不存在这个索引，退化为间隙锁
2. 索引上的等值查询，向右遍历时且最后⼀个值不满⾜等值条件的时候，next-key lock 退化为间隙锁

**注意**：

1. ⾮唯⼀索引的范围查询：范围查询都会访问到不满⾜条件的第⼀个值，并且不会执⾏上述的两个优化。唯⼀索引的范围查询仍旧会执⾏上述的优化
2. delete 语句的加锁方式和查询相同
3. limit 语句，遍历到满足条件的 n 条数据后，之后不再加 next-key lock



**死锁：不同线程可以给同一个间隙加锁。**

意思时只有我这个线程可以操控这个间隙，其他线程不能使⽤，加锁时不会检测是否冲突的，但是如果两个都给同⼀个间隙上锁，之后两个线程都没办法在这个间隙上更新数据了，都会陷⼊等待另⼀个线程的间隙锁释放，也就是死锁。

如果使⽤**读提交**隔离级别，那么**只加⾏锁，不加间隙锁**，语句执⾏过程中加上的⾏锁，在语句执⾏完成后，就要把“不满⾜条件的⾏”上的⾏锁直接释放了，不需要等到事务提交才释放。

## 7、for update 的使用场景

⼀般这些操作都是很长⼀串并且是开启事务的

如果库存刚开始读的时候是1，⽽⽴马另⼀个进程进⾏了update将库存更新为0了，⽽事务还没有结束，会将错的数据⼀直执⾏下去，就会有问题

需要 for update 进⾏数据加锁防⽌⾼并发时候数据出错，即使事务保持 **当前读** 状态

```mysql
# 如果只给定一个参数，表示记录数
select * from table limit [offset,] rows | rows offset offset;
```

```mysql
select * from orange limit 5
```

相当于

```mysql
select * from orange limit 0, 5;
```

两个参数，第一个参数表示 offset，第二个参数为记录数

```mysql
select * from orange limit 10, 15; # 检索记录 11-25
```



如果想要清除⼀些 MySQL 使⽤内部缓存，使⽤ FLUSH 命令

## 8、事务隔离

innodb ⽀持 RC 和 RR 隔离级别实现是⽤的⼀致性视图(consistent read view)

事务在启动时会拍⼀个快照，这个快照是基于整个库的

==如果在事务内 select t 表,另外的事务执⾏了DDL t表,根据发⽣时间,只有两种情况：==

1. ==报错==
2. ==锁住==

## 9、事务如何实现 mvcc？

1. 每个事务都有一个事务 ID，叫作 transaction id（严格递增）

2. 事务在启动时，找到已提交的最大事务 ID 记为 up_limit_id

3. 事务在更新一条语句时

   ⽐如 id=1 改为了 id=2.会把 id=1 和该⾏之前的 row trx_id 写到 undo log ⾥, 并且在数据页上把 id 的值改为 2,并且把修改这条语句的transaction id 记在该⾏⾏头

4. 再定一个规矩

   ⼀个事务要查看⼀条数据时,必须先⽤该事务的 up_limit_id 与该⾏的 transaction id 做⽐对

   - 如果 up_limit_id >= transaction id，那么可以看
   - 如果 up_limit_id < transaction id,则只能去 undo log ⾥去取
   - 去 undo log 查找数据的时候,也需要做⽐对,必须 up_limit_id > transaction id，才返回数据

## 10、当前读

由于当前读都是先读后写，只能读当前的值，所以当前读会更新事务内的 up_limit_id 为该
事务的 transaction id

## 11、DB_ROW_ID 作用是什么？假设有10个 update，到第九个回滚了，DB_ROLL_PTR 如何做的，那提交了是否更新 DB_ROLL_PTR？



## 12、为什么 rr 能实现可重复读⽽ rc 不能，分两种情况

1. **快照读情况**：

   rr 不能更新事务内的 up_limit_id ，⽽ rc 每次会把 up_limit_id 更新为快照读之前最新已提交
   事务的 transaction id，则 rc 不能可重复读

2. **当前读情况**：

   rr 是利⽤ record lock+gap lock来实现的，⽽ rc 没有 gap，所以 rc 不能可重复读

## 13、选择普通索引还是唯一索引？

1. **对于查询过程来说**：

   1. **普通索引**

      查到满足条件的第一个记录后，继续查找下一个记录，直到第一个不满足条件的记录

   2. **唯一索引**

      由于索引的唯一性，查到第一个满足条件的记录后，停止检索

   3. 两者的性能差距微乎其微。因为 InnoDB 根据数据页来读写的。

2. **对于更新过程来说**：

   **change buffer**

   当需要更新一个数据页：

   - 如果数据页在内存中就直接更新
   - 如果不在内存中，在不影响数据⼀致性的前提下，InnoDB 会将这些更新操作缓存在change buffer 中

   下次查询需要访问这个数据页的时候，将数据页读⼊内存，然后执⾏ change buffer 中的与这个页有关的操作

   change buffer 是可以持久化的数据。在内存中有拷贝，也会被写⼊到磁盘上

   **purge**：将 change buffer 中的操作应⽤到原数据页上，得到最新结果的过程，称为 purge

   访问这个数据页会触发 purge，系统有后台线程定期 purge，在数据库正常关闭的过程中，
   也会执⾏ purge

   **唯一索引的更新不能使用 change buffer**

   change buffer ⽤的是 buffer pool ⾥的内存，change buffer 的⼤⼩，可以通过参数 innodb_change_buffer_max_size 来动态设置。这个参数设置为50的时候，表示 change buffer 的⼤⼩最多只能占⽤ buffer pool 的50%。

   将数据从磁盘读⼊内存涉及随机IO的访问，是数据库⾥⾯成本最⾼的操作之⼀

   change buffer 因为减少了随机磁盘访问，所以对更新性能的提升很明显

## 14、change buffer 使用场景

在⼀个数据页做 purge 之前，change buffer 记录的变更越多，收益就越⼤。

对于写多读少的业务来说，页⾯在写完以后马上被访问到的概率⽐较⼩，此时 change
buffer 的使⽤效果最好。这种业务模型常见的就是**账单类、⽇志类**的系统。

反过来，假设⼀个业务的更新模式是写⼊之后马上会做查询，那么即使满⾜了条件，将更新
先记录在 change buffer，但之后由于马上要访问这个数据页，会⽴即触发 purge 过程。

这样随机访问 IO 的次数不会减少，反⽽增加了 change buffer 的维护代价。所以，对于这种
业务模式来说，change buffer 反而起到了副作⽤。

## 15、索引的选择和实践

尽可能使用普通索引

## 16、redo log 和 change buffer 的区别

- redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写）
- change buffer 主要节省的是随机读磁盘的 IO 消耗

## 17、char 和 varchar 的区别

char 是固定长度类型，varchar 是可变长度类型



**存储情况不同**

- **以 compact 行格式为例**：

一条完整的记录由两部分组成：

1. 变长字段的真正数据内容
2. 该变长字段所占用的字节数

该变长字段所占用的字节数被放在变长字段列表中，并按照列的顺序逆序存放

而 char 类型的数据则只需要存储其真正的数据内容

- **对于类型为 char(M) 的列**

如果采用的是变长编码的字符集：

那么该列的值占用的字节数也会被存储到变长字段列表中，采用变长编码字符集的 char(M) 类型的值要求至少占用 M 个字节，但 varchar(M) 没有这一要求

## 18、MySQL 抖一下是什么意思

因为运行的不正常了，或者不稳定了，要花费更多的资需源处理别的事情，会使SQL语句的执行效率明显变慢。

**脏页**：和磁盘数据不一致的页成为脏页

针对 innoDB 导致 MySQL 抖的原因，主要是 InnoDB 会在后台刷脏页，而刷脏页的过程是要将内存页写⼊磁盘。所以，⽆论是你的查询语句在需要内存的时候可能要求淘汰⼀个脏页，还是由于刷脏页的逻辑会占⽤ IO 资源并可能影响到了你的更新语句，都可能是造成你从业务端感知 MySQL “抖”了⼀下的原因。

## 19、MySQL 抖一下有啥问题

系统不稳定，性能突然下降对业务端不友好

## 20、怎么让 MySQL 不抖

设置合理参数配配置，尤其是设置好 innodb_io_capacity 的值，并且平时要多关注脏页比例，不要让它经常接近 75%

## 21、什么是脏页

当内存数据页跟磁盘数据页内容不⼀致的时候，我们称这个内存页为“脏页”。

## 22、什么是干净页

内存数据写⼊到磁盘后，内存和磁盘上的数据页的内容就⼀致了，称为“干净页”。

## 23、脏页是怎么产生的

因为使⽤了 WAL 技术，这个技术会把数据库的**随机写**转化为**顺序写**，但副作⽤就是会产⽣脏页。比如常见的 redo 和 undo 日志。

## 24、什么是随机写？为什么那么耗性能？

随机写我的理解是，这次写磁盘的那个扇区和上⼀次没啥关系，需要重新定位位置，机械运动是很慢的即使不是机械运动重新定位写磁盘的位置也是很耗时的。

## 25、什么是顺序写？

顺序写我的理解是，这次写磁盘那个扇区就在上⼀次的下⼀个位置，不需要重新定位写磁盘的位置速度当然会快⼀些。

## 26、WAL 怎么把随机写转化为顺序写的？

写 redolog 是顺序写的，先写 redolog 等合适的时候再写磁盘，间接的将随机写变成了顺序写，性能确实会提⾼不少。

## 27、为什么删除了表的一半数据，表文件大小没变化？

因为 delete 命令其实只是把记录的位置，或者数据页标记为了“可复⽤”，但磁盘⽂件的大小是不会变的。也可以认为是⼀种逻辑删除，所以物理空间没有实际释放，只是标记为可复⽤，表⽂件的大小当然是不变的啦

## 28、表的数据信息存在哪里？

共享表空间里或以单独存储在⼀个以 .ibd 为后缀的⽂件⾥，由参数 innodb_file_per_table 来控制。

建议总是作为⼀个单独的⽂件来存储，这样非常容易管理，并且在不需要的时候，使⽤ drop table 命令也能直接把对应的⽂件删除，如果存储在共享空间之中即使表删除了空间也不会释放。

## 29、表的结构信息存在哪里？

⾸先，表结构定义占有的存储空间比较小

MySQL8.0 之前：表结构的定义信息存在以.frm为后缀的⽂件里

MySQL8.0 之后：则允许把表结构的定义信息存储在**系统数据表**之中

系统数据表，主要⽤于存储MySQL的系统数据，⽐如：数据字典、undo log(默认)等⽂件

## 30、如何才能删除表数据后，表文件大小就变小？

重建表，消除表因为进⾏⼤量的增删改操作而产⽣的空洞，使⽤如下命令：

```mysql
alter table t engine=InnoDB
optimize table t	# 优化表
truncate table t	# 删除表的数据，不删除表结构
```

## 31、空洞是什么？怎么产生的？

空洞就是被标记**可复用但还没被使用**的存储空间。



**产生原因**：

1. 使用 delete 命令删除数据会产生空洞，标记为可复用
2. 插入新的数据可能引起页分裂，可能产生空洞
3. 修改操作，有时是一种先删后插的操作也可能产生空洞

## 32、count(*) 这么慢，该怎么办？

要么忍，要么手动记录

## 33、count() 的语义是什么？

**针对 innodb 来说**

count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加1，否则不加，然后累计计数

**不同的存储引擎实现方式不同**

- MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高；
- InnoDB 引擎就麻烦了，它执行 count(*) 的时候，需要把数据一行一行地从引擎里读出来，然后累积计数

## 34、count(字段) 怎么计数？

1. 如果这个字段定义为 not null 的话，

   一行行从记录里面读出这个字段，判断不能为 null，按行累加；

2. 如果这个字段定义为允许为 null，

   执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加

从引擎返回的字段会涉及到解析数据行，以及拷贝字段值的操作

## 35、count(主键 id) 怎么计数？

对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每⼀⾏的 id 值都取出来，返回给 server 层。

server 层拿到 id 后，判断是不可能为空的，就按⾏累加。从引擎返回的 主键id 会涉及到解析数据⾏，以及拷贝字段值的操作。

## 36、count(1) 怎么计数？

对于 count(1) 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每⼀⾏，放⼀个数字“1”进去，判断是不可能为空的，按⾏累加。

## 37、count(*) 怎么计数？

对于count(*)来说，并不会把全部字段取出来，⽽是专门做了优化，不取值。

count(*) 肯定不是 null，按⾏累加。

**这些 count() 的原则如下**：

server 层要什么就给什么；

InnoDB 只给必要的值；

现在的优化器只优化了 count(*) 的语义为“取行数”，其它“显而易见”的优化并没有做

## 38、order by 是怎样工作的？

**example**

```mysql
select city, name, age from t where city = '杭州' order by name limit 1000;
```

涉及到用户语句的排序，mysql 会给每个线程分配一块内存用于排序，也就是 sort_buffer。

**执行逻辑**：

1. 先初始化 sort_buffer
2. 然后放入 city，name，age 字段，不断地由主键 id 索引到整行再到三个字段的值，匹配查找的值存入 sort_buffer
3. 然后按 name 排序，返回前 1000 个值

但是，如果 sort_buffer_size 设置的太小，无法存放所有匹配的字段，排序就无法在内存中完成。此时，需要借助磁盘临时文件辅助排序，可以通过 number_of_tmp_files 这个标识来判断是否使用，这个原理和对超大数据的排序相同。

如果要记录的字段太长，内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差。

这时，会换个算法，叫作 rowid 排序。就是对主键 id 以及排序字段进行存放，这样就节省了空间，但是最后需要通过主键 id 去找到之前未取出的字段。对比全字段排序，rowid 排序多访问了一个表 t 的主键索引。

**结论**：

如果 MySQL 实在是担⼼排序内存太小，会影响排序效率，才会采⽤ rowid 排序算法，这样排序过程中⼀次可以排序更多行，但是需要再回到原表去取数据。

如果 MySQL 认为内存⾜够⼤，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存⾥⾯返回查询结果了，不⽤再回到原表去取数据。

如果想要避免排序，可以建⽴对应字段的索引。

如果想要进⼀步避免主键 id 的回表查询，可以使⽤覆盖索引，这种情况的索引建⽴成本会⽐较⼤，需要权衡是否使⽤。

## 39、如何正确地显示随机消息？

> **example**：
>
> 英语学习 App ⾸页有⼀个随机显⽰单词的功能，也就是根据每个⽤户的级别有⼀个单词表，然后这个⽤户每次访问⾸页的时候，都会随机滚动显⽰三个单词。他们发现随着单词表变⼤，选单词这个逻辑变得越来越慢，甚⾄影响到了⾸页的打开速度

1. **order by rand() 实现**

   创建临时表->按主键顺序取出所有的单词，并给他们⼀个随机小数→初始化 sort_buffer，从内存临时表取出数据放⼊ sort_buffer，按随机数排序，取出前三个，总扫描⾏数是2×n+3
